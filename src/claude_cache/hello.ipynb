{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, world!\")\n",
    "\n",
    "#  运行具有“.venv (Python 3.12.4)”的单元格需要ipykernel包。\n",
    "# 运行以下命令，将 \\\"ipykernel\\\" 安装到 Python 环境中。\n",
    "# 命令: \\\"/Users/davirian/dev/claude-example/.venv/bin/python -m pip install ipykernel -U --force-reinstall\\\"\"\n",
    "\n",
    "# 安装ipykernel\n",
    "#uv pip install ipykernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检索增强生成（Retrieval Augmented Generation，简称RAG）\n",
    "\n",
    "\n",
    "Claude（一种AI模型）在许多任务中表现出色，但在处理特定于您独特业务环境的查询时可能会遇到困难。这就是RAG变得非常有价值的原因。RAG使Claude能够利用您的内部知识库或客户支持文档，显著提高其回答特定领域问题的能力。越来越多的企业正在构建RAG应用程序，以改善客户支持、内部公司文档问答、财务和法律分析等方面的工作流程。\n",
    "\n",
    "在本指南中，我们将演示如何使用Anthropic文档作为知识库来构建和优化RAG系统。我们将引导您完成以下步骤：\n",
    "\n",
    "1. 使用内存向量数据库和Voyage AI的嵌入来设置基本的RAG系统。\n",
    "2. 建立一个健全的评估套件。我们将超越基于\"直觉\"的评估，向您展示如何独立测量检索管道和端到端性能。\n",
    "3. 实施先进技术来改进RAG，包括摘要索引和使用Claude进行重新排序。\n",
    "\n",
    "通过一系列有针对性的改进，与基本RAG管道相比，我们在以下指标上取得了显著的性能提升（稍后我们会解释这些指标的含义）：\n",
    "\n",
    "平均精确度：从0.43提升到0.46\n",
    "平均召回率：从0.66提升到0.74\n",
    "平均F1分数：从0.52提升到0.57\n",
    "平均平均倒数排名（MRR）：从0.74提升到0.93\n",
    "端到端准确率：从70%提升到83%\n",
    "\n",
    "\n",
    ">**注意事项：**\n",
    ">本指南中的评估旨在模拟生产环境中的评估系统，您应该注意到运行这些评估可能需要一些时间。另外值得注意的是：如果您完整运行评估，除非您处于第二级或更高级别，否则可能会遇到速率限制。如果您想节省令牌使用量，可以考虑跳过完整的端到端评估。\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. 设置\n",
    "2. 基本 RAG - 第 1 级\n",
    "3. 建立评估系统\n",
    "4. 二级 - 摘要索引\n",
    "5. 3 级 - 摘要索引和重新排序\n",
    "\n",
    "## 设置\n",
    "\n",
    "我们需要一些库,包括:\n",
    "\n",
    "1. anthropic - 与 Claude 交互\n",
    "2. voyageai - 生成高质量嵌入\n",
    "3. pandas, numpy, matplotlib, and scikit-learn 用于数据处理和可视化\n",
    "\n",
    "您还需要从[Anthropic](https://www.anthropic.com/)和[Voyage AI](https://www.voyageai.com/)获取 API 密钥。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup\n",
    "!uv pip install anthropic\n",
    "!uv pip install voyageai\n",
    "!uv pip install pandas\n",
    "!uv pip install numpy\n",
    "!uv pip install matplotlib\n",
    "!uv pip install seaborn\n",
    "!uv pip install -U scikit-learn\n",
    "!uv pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['VOYAGE_API_KEY'] = os.getenv(\"VOYAGE_API_KEY\")\n",
    "os.environ['ANTHROPIC_API_KEY'] = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(\"Voyage API Key:\", os.environ['VOYAGE_API_KEY'])\n",
    "print(\"Anthropic API Key:\", os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化矢量数据库类\n",
    "\n",
    "在此示例中,我们正在使用内存向量数据库,但是对于生产应用程序,您可能需要使用托管解决方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, data):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts = [f\"Heading: {item['chunk_heading']}\\n\\n Chunk Text:{item['text']}\" for item in data]\n",
    "        self._embed_and_store(texts, data)\n",
    "        self.save_db()\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.client.embed(\n",
    "                texts[i : i + batch_size],\n",
    "                model=\"voyage-2\"\n",
    "            ).embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        self.save_db()\n",
    "        return top_examples\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本 RAG - 第 1 级\n",
    "\n",
    "要开始,我们将使用简单的方法建立一个基本的 RAG 管道。这在业内有时被称为\"朴素 RAG\"。一个基本的 RAG 管道包括以下 3 个步骤:\n",
    "\n",
    "1. 按标题分块文档 - 仅包含每个子标题下的内容\n",
    "2. 嵌入每个文档\n",
    "3. 使用余弦相似度来检索文档以回答查询\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load the Anthropic documentation\n",
    "with open('data/anthropic_docs.json', 'r') as f:\n",
    "    anthropic_docs = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "db = VectorDB(\"anthropic_docs\")\n",
    "db.load_data(anthropic_docs)\n",
    "\n",
    "def retrieve_base(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n{chunk['text']}\\n\"\n",
    "    return results, context\n",
    "\n",
    "def answer_query_base(query, db):\n",
    "    documents, context = retrieve_base(query, db)\n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2500,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估设置\n",
    "\n",
    "在评估基于检索增强的生成模型(RAG)应用程序时,分别评估检索系统和端到端系统的性能非常关键。\n",
    "\n",
    "我们合成生成了一个由 100 个样本组成的评估数据集,其中包括以下内容:\n",
    "\n",
    "- 一个问题\n",
    "- 与此问题相关的我们文档中的部分内容。这就是我们期望检索系统在被问到这个问题时检索到的内容。\n",
    "- 正确答案。\n",
    "\n",
    "这是一个较有挑战性的数据集。我们的一些问题需要在多个片段间进行综合分析才能得到正确答案,因此我们的系统需要同时加载多个片段。你可以通过打开 evaluation/docs_evaluation_dataset.json 来查看数据集。\n",
    "\n",
    "运行下一个单元格以预览数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previewing our eval dataset\n",
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=3):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "preview_json('evaluation/docs_evaluation_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Definitions \n",
    "\n",
    "我们将根据 5 个关键指标来评估我们的系统:精确度、召回率、F1 分数、平均倒数排名(MRR)和端到端准确性。\n",
    "\n",
    "### 检索指标:\n",
    "\n",
    "#### 精确\n",
    "\n",
    "精确性代表被检索到的块中实际相关的比例。它回答了这个问题：\"在我们检索到的块中,有多少是正确的?\"\n",
    "\n",
    "\n",
    "关键要点:\n",
    "\n",
    "- 高精度表示一个高效的系统,很少出现误报。\n",
    "- 低精度表示正在检索到许多无关的块。\n",
    "- 我们的系统每次查询至少检索 3 个片段,这可能会影响精确度评分。\n",
    "\n",
    "#### Recall\n",
    "\n",
    "召回率衡量了我们检索系统的完整性。它回答了这个问题:\"在所有正确存在的块中,我们成功检索了多少个?\"\n",
    "\n",
    "关键要点:\n",
    "\n",
    "- 高回召意味着对必要信息的全面覆盖。\n",
    "- 低召回率表明遗漏了重要部分。\n",
    "- 确保LLM能够访问所有所需信息的关键在于回忆。\n",
    "\n",
    "#### F1 分数\n",
    "\n",
    "F1 分数提供了精确度和召回率之间的平衡度量。当需要一个单一的指标来评估系统性能时,尤其是在不平衡的类别分布情况下,它特别有用。\n",
    "\n",
    "关键要点:\n",
    "\n",
    "- F1 得分范围从 0 到 1,其中 1 代表完美的精确度和召回率。\n",
    "- 它是精确度和召回率的谐波平均值，趋向于两个值中较低的那个。\n",
    "- 在假阳性和假阴性都很重要的场景中很有用。\n",
    "\n",
    "解释 F1 分数:\n",
    "\n",
    "- F1 分数 1.0 表示完美的精确度和召回率。\n",
    "- An F1 score of 0.0 indicates the worst performance.\n",
    "- 总的来说,F1 分数越高,整体表现越好。\n",
    "\n",
    "### 平衡精确率、召回率和 F1 得分\n",
    "\n",
    "- 精确度和召回率之间通常存在权衡。\n",
    "- 我们的系统最小块检索更偏重于召回率而非精确率。\n",
    "- 最优平衡取决于具体用例。\n",
    "- 在许多 RAG 系统中,高召回率通常是优先考虑的,因为LLMs可以在生成过程中过滤掉不太相关的信息。\n",
    "\n",
    "## 平均倒数排名（MRR）@k\n",
    "\n",
    "MRR 衡量我们的系统对相关信息的排名效果。它帮助我们了解用户从检索结果的最顶端开始,需要多快才能找到他们想要的内容。\n",
    "\n",
    "关键要点:\n",
    "\n",
    "- MRR 的范围从 0 到 1,其中 1 是完美的(正确答案总是第一位的)。\n",
    "- 它只考虑每个查询中第一个正确结果的排名。\n",
    "- 较高的最小相关度表示相关信息的排序更好。\n",
    "\n",
    "在哪里：\n",
    "\n",
    "- |Q|是查询的总数\n",
    "- rank_i 是第 i 个查询的第一个相关项目的位置\n",
    "\n",
    "## 端到端指标:\n",
    "\n",
    "###  端到端准确性\n",
    "\n",
    "我们使用LLM作为评判者（Claude 3.5 Sonnet）来评估生成的答案是否正确,这是基于问题和真实答案。\n",
    "\n",
    "这个指标评估了从检索到答案生成的整个管道。\n",
    "\n",
    "## 定义我们的度量计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correect answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format:\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20240620\",\n",
    "                max_tokens=1500,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": \"<evaluation>\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                stop_sequences=[\"</evaluation>\"]\n",
    "            )\n",
    "            \n",
    "            response_text = response.content[0].text\n",
    "            print(response_text)\n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct = evaluation.find('is_correct').text.lower() == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于绘制性能的辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_performance(results_folder='evaluation/json_results', include_methods=None, colors=None):\n",
    "    # Set default colors\n",
    "    default_colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "    if colors is None:\n",
    "        colors = default_colors\n",
    "    \n",
    "    # Load JSON files\n",
    "    results = []\n",
    "    for filename in os.listdir(results_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(results_folder, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if 'name' not in data:\n",
    "                        print(f\"Warning: {filename} does not contain a 'name' field. Skipping.\")\n",
    "                        continue\n",
    "                    if include_methods is None or data['name'] in include_methods:\n",
    "                        results.append(data)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: {filename} is not a valid JSON file. Skipping.\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No JSON files found with matching 'name' fields.\")\n",
    "        return\n",
    "    \n",
    "    # Validate data\n",
    "    required_metrics = [\"average_precision\", \"average_recall\", \"average_f1\", \"average_mrr\", \"end_to_end_accuracy\"]\n",
    "    for result in results.copy():\n",
    "        if not all(metric in result for metric in required_metrics):\n",
    "            print(f\"Warning: {result['name']} is missing some required metrics. Skipping.\")\n",
    "            results.remove(result)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid results remaining after validation.\")\n",
    "        return\n",
    "    \n",
    "    # Sort results based on end-to-end accuracy\n",
    "    results.sort(key=lambda x: x['end_to_end_accuracy'])\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    methods = [result['name'] for result in results]\n",
    "    metrics = required_metrics\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    x = range(len(metrics))\n",
    "    width = 0.8 / len(results)\n",
    "    \n",
    "    # Create color palette\n",
    "    num_methods = len(results)\n",
    "    color_palette = colors[:num_methods] + sns.color_palette(\"husl\", num_methods - len(colors))\n",
    "    \n",
    "    # Plot bars for each method\n",
    "    for i, (result, color) in enumerate(zip(results, color_palette)):\n",
    "        values = [result[metric] for metric in metrics]\n",
    "        offset = (i - len(results)/2 + 0.5) * width\n",
    "        bars = plt.bar([xi + offset for xi in x], values, width, label=result['name'], color=color)\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Metrics', fontsize=12)\n",
    "    plt.ylabel('Values', fontsize=12)\n",
    "    plt.title('RAG Performance Metrics (Sorted by End-to-End Accuracy)', fontsize=16)\n",
    "    plt.xticks(x, metrics, rotation=45, ha='right')\n",
    "    plt.legend(title='Methods', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估我们的基本情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs = evaluate_retrieval(retrieve_base, eval_data, db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_base, db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('evaluation/csvs/evaluation_results_detailed.csv', index=False)\n",
    "print(\"Detailed results saved to evaluation/csvs/evaluation_results_one.csv\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "with open('evaluation/json_results/evaluation_results_one.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Basic RAG\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to evaluation_results_one.json, evaluation_results_one.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualize our performance\n",
    "plot_performance('evaluation/json_results', ['Basic RAG'], colors=['skyblue'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
